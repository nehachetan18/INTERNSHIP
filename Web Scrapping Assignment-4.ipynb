{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c3dfff",
   "metadata": {},
   "source": [
    "# WEB SCRAPPING ASSIGNMENT-4"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80af055c",
   "metadata": {},
   "source": [
    "1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details: A) Rank B) Name C) Artist D)\n",
    "Upload date E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b26ba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f09032dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WebDriver (make sure you have the Chrome WebDriver executable)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b347a38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rank                                             Name  \\\n",
      "0   1.                            \"Baby Shark Dance\"[6]   \n",
      "1   2.                                   \"Despacito\"[9]   \n",
      "2   3.                       \"Johny Johny Yes Papa\"[17]   \n",
      "3   4.                                  \"Bath Song\"[18]   \n",
      "4   5.                               \"Shape of You\"[19]   \n",
      "5   6.                              \"See You Again\"[22]   \n",
      "6   7.                          \"Wheels on the Bus\"[27]   \n",
      "7   8.                \"Phonics Song with Two Words\"[28]   \n",
      "8   9.                                \"Uptown Funk\"[29]   \n",
      "9  10.  \"Learning Colors – Colorful Eggs on a Farm\"[30]   \n",
      "\n",
      "                                              Artist        Upload_date  Views  \n",
      "0        Pinkfong Baby Shark - Kids' Songs & Stories      June 17, 2016  13.48  \n",
      "1                                         Luis Fonsi   January 12, 2017   8.28  \n",
      "2  LooLoo Kids - Nursery Rhymes and Children's Songs    October 8, 2016   6.82  \n",
      "3                         Cocomelon - Nursery Rhymes        May 2, 2018   6.45  \n",
      "4                                         Ed Sheeran   January 30, 2017   6.11  \n",
      "5                                        Wiz Khalifa      April 6, 2015   6.05  \n",
      "6                         Cocomelon - Nursery Rhymes       May 24, 2018   5.62  \n",
      "7              ChuChu TV Nursery Rhymes & Kids Songs      March 6, 2014   5.52  \n",
      "8                                        Mark Ronson  November 19, 2014   5.05  \n",
      "9                                        Miroshka TV  February 27, 2018   4.99  \n"
     ]
    }
   ],
   "source": [
    "# Create dictionaries to store data\n",
    "Most_Viewed = {\n",
    "    'Rank': [],\n",
    "    'Name': [],\n",
    "    'Artist': [],\n",
    "    'Upload_date': [],\n",
    "    'Views': []\n",
    "}\n",
    "\n",
    "# Wait for the table to load (adjust the time.sleep as needed)\n",
    "time.sleep(5)\n",
    "\n",
    "# Get the page source after waiting\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Parse the page source with BeautifulSoup\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "# Find the table with the video details\n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "# Loop through the rows of the table\n",
    "for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "    columns = row.find_all('td')\n",
    "    if len(columns) >= 5:\n",
    "        rank = columns[0].get_text(strip=True)\n",
    "        name = columns[1].get_text(strip=True)\n",
    "        artist = columns[2].get_text(strip=True)\n",
    "        upload_date = columns[4].get_text(strip=True)\n",
    "        views = columns[3].get_text(strip=True)\n",
    "\n",
    "        Most_Viewed['Rank'].append(rank)\n",
    "        Most_Viewed['Name'].append(name)\n",
    "        Most_Viewed['Artist'].append(artist)\n",
    "        Most_Viewed['Upload_date'].append(upload_date)\n",
    "        Most_Viewed['Views'].append(views)\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "Most_Viewed_df = pd.DataFrame(Most_Viewed)\n",
    "\n",
    "# Display the first 10 rows\n",
    "print(Most_Viewed_df.head(10))\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63123aa3",
   "metadata": {},
   "source": [
    "2. Scrape the details team India’s international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Series\n",
    "B) Place\n",
    "C) Date\n",
    "D) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e754a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 'NoneType' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.bcci.tv/\"\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find the link to the international fixtures page\n",
    "        fixtures_link = soup.find('a', text='International Fixtures')['href']\n",
    "        \n",
    "        # Construct the full URL for the fixtures page\n",
    "        fixtures_url = url + fixtures_link\n",
    "        \n",
    "        # Send another GET request to the fixtures page\n",
    "        fixtures_response = requests.get(fixtures_url)\n",
    "        \n",
    "        if fixtures_response.status_code == 200:\n",
    "            # Parse the HTML content of the fixtures page\n",
    "            fixtures_soup = BeautifulSoup(fixtures_response.content, 'html.parser')\n",
    "            \n",
    "            # Find all fixture items on the page\n",
    "            fixture_items = fixtures_soup.find_all('li', class_='fixture-list__item')\n",
    "            \n",
    "            for item in fixture_items:\n",
    "                series = item.find('span', class_='u-unskewed-text').text.strip()\n",
    "                place = item.find('p', class_='fixture__additional-info').text.strip()\n",
    "                date_time = item.find('div', class_='fixture__datetime desktop-only').text.strip()\n",
    "                \n",
    "                print(\"Series:\", series)\n",
    "                print(\"Place:\", place)\n",
    "                print(\"Date and Time:\", date_time)\n",
    "                print(\"------------------------\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfae0d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "International Fixtures link not found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.bcci.tv/\"\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the link to the international fixtures page\n",
    "    fixtures_link = soup.find('a', text='International Fixtures')\n",
    "    \n",
    "    if fixtures_link:\n",
    "        fixtures_link = fixtures_link['href']\n",
    "        \n",
    "        # Construct the full URL for the fixtures page\n",
    "        fixtures_url = url + fixtures_link\n",
    "        \n",
    "        # Send another GET request to the fixtures page\n",
    "        fixtures_response = requests.get(fixtures_url)\n",
    "        \n",
    "        fixtures_response.raise_for_status()\n",
    "        \n",
    "        # Parse the HTML content of the fixtures page\n",
    "        fixtures_soup = BeautifulSoup(fixtures_response.content, 'html.parser')\n",
    "        \n",
    "        # Find all fixture items on the page\n",
    "        fixture_items = fixtures_soup.find_all('li', class_='fixture-list__item')\n",
    "        \n",
    "        for item in fixture_items:\n",
    "            series = item.find('p', class_='fixture__additional-info').text.strip()\n",
    "            place = item.find('span', class_='fixture__additional-info').text.strip()\n",
    "            date_time = item.find('div', class_='fixture__datetime desktop-only').text.strip()\n",
    "            \n",
    "            print(\"Series:\", series)\n",
    "            print(\"Place:\", place)\n",
    "            print(\"Date and Time:\", date_time)\n",
    "            print(\"------------------------\")\n",
    "    else:\n",
    "        print(\"International Fixtures link not found.\")\n",
    "\n",
    "except requests.exceptions.HTTPError as errh:\n",
    "    print (\"HTTP Error:\", errh)\n",
    "except requests.exceptions.ConnectionError as errc:\n",
    "    print (\"Error Connecting:\", errc)\n",
    "except requests.exceptions.Timeout as errt:\n",
    "    print (\"Timeout Error:\", errt)\n",
    "except requests.exceptions.RequestException as err:\n",
    "    print (\"Something went wrong:\", err)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c476a581",
   "metadata": {},
   "source": [
    "3. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details: A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29a65acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Importing selenium webdriver \n",
    "from selenium import webdriver\n",
    "\n",
    "# Importing required Exceptions which needs to handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "#Importing requests\n",
    "import requests\n",
    "\n",
    "# importing regex\n",
    "import re\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c2a3ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State</th>\n",
       "      <th>Share In GDP</th>\n",
       "      <th>GDP of State</th>\n",
       "      <th>GSDP_Current</th>\n",
       "      <th>GSDP_Previous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>399.921</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>2,039,074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>8.63%</td>\n",
       "      <td>247.629</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>1,215,307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>240.726</td>\n",
       "      <td>1,584,764</td>\n",
       "      <td>1,123,982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>7.96%</td>\n",
       "      <td>228.290</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>1,186,379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>7.91%</td>\n",
       "      <td>226.806</td>\n",
       "      <td>1,493,127</td>\n",
       "      <td>1,091,077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>5.77%</td>\n",
       "      <td>165.556</td>\n",
       "      <td>1,089,898</td>\n",
       "      <td>739,525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>4.99%</td>\n",
       "      <td>143.179</td>\n",
       "      <td>942,586</td>\n",
       "      <td>677,428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>4.57%</td>\n",
       "      <td>131.083</td>\n",
       "      <td>862,957</td>\n",
       "      <td>621,301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>4.56%</td>\n",
       "      <td>130.791</td>\n",
       "      <td>861,031</td>\n",
       "      <td>612,828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>4.29%</td>\n",
       "      <td>122.977</td>\n",
       "      <td>809,592</td>\n",
       "      <td>522,009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>4.14%</td>\n",
       "      <td>118.733</td>\n",
       "      <td>781,653</td>\n",
       "      <td>559,412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>4.10%</td>\n",
       "      <td>117.703</td>\n",
       "      <td>774,870</td>\n",
       "      <td>590,569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>3.89%</td>\n",
       "      <td>111.519</td>\n",
       "      <td>734,163</td>\n",
       "      <td>531,085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>2.81%</td>\n",
       "      <td>80.562</td>\n",
       "      <td>530,363</td>\n",
       "      <td>375,651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>2.79%</td>\n",
       "      <td>79.957</td>\n",
       "      <td>526,376</td>\n",
       "      <td>397,669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>2.58%</td>\n",
       "      <td>74.098</td>\n",
       "      <td>487,805</td>\n",
       "      <td>376,877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>1.67%</td>\n",
       "      <td>47.982</td>\n",
       "      <td>315,881</td>\n",
       "      <td>234,048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>1.61%</td>\n",
       "      <td>46.187</td>\n",
       "      <td>304,063</td>\n",
       "      <td>231,182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>1.57%</td>\n",
       "      <td>45.145</td>\n",
       "      <td>297,204</td>\n",
       "      <td>224,986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>1.30%</td>\n",
       "      <td>37.351</td>\n",
       "      <td>245,895</td>\n",
       "      <td>193,273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir</td>\n",
       "      <td>0.83%</td>\n",
       "      <td>23.690</td>\n",
       "      <td>155,956</td>\n",
       "      <td>112,755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>0.81%</td>\n",
       "      <td>23.369</td>\n",
       "      <td>153,845</td>\n",
       "      <td>117,851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>0.39%</td>\n",
       "      <td>11.115</td>\n",
       "      <td>73,170</td>\n",
       "      <td>57,787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>0.26%</td>\n",
       "      <td>7.571</td>\n",
       "      <td>49,845</td>\n",
       "      <td>36,963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>6.397</td>\n",
       "      <td>42,114</td>\n",
       "      <td>31,192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.230</td>\n",
       "      <td>34,433</td>\n",
       "      <td>23,013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.086</td>\n",
       "      <td>33,481</td>\n",
       "      <td>24,682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.363</td>\n",
       "      <td>28,723</td>\n",
       "      <td>18,722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.233</td>\n",
       "      <td>27,870</td>\n",
       "      <td>19,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.144</td>\n",
       "      <td>27,283</td>\n",
       "      <td>17,647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>3.737</td>\n",
       "      <td>24,603</td>\n",
       "      <td>16,676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>3.385</td>\n",
       "      <td>22,287</td>\n",
       "      <td>16,478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                      State Share In GDP GDP of State GSDP_Current  \\\n",
       "0     1                Maharashtra       13.94%      399.921    2,632,792   \n",
       "1     2                 Tamil Nadu        8.63%      247.629    1,630,208   \n",
       "2     3              Uttar Pradesh        8.39%      240.726    1,584,764   \n",
       "3     4                    Gujarat        7.96%      228.290    1,502,899   \n",
       "4     5                  Karnataka        7.91%      226.806    1,493,127   \n",
       "5     6                West Bengal        5.77%      165.556    1,089,898   \n",
       "6     7                  Rajasthan        4.99%      143.179      942,586   \n",
       "7     8             Andhra Pradesh        4.57%      131.083      862,957   \n",
       "8     9                  Telangana        4.56%      130.791      861,031   \n",
       "9    10             Madhya Pradesh        4.29%      122.977      809,592   \n",
       "10   11                     Kerala        4.14%      118.733      781,653   \n",
       "11   12                      Delhi        4.10%      117.703      774,870   \n",
       "12   13                    Haryana        3.89%      111.519      734,163   \n",
       "13   14                      Bihar        2.81%       80.562      530,363   \n",
       "14   15                     Punjab        2.79%       79.957      526,376   \n",
       "15   16                     Odisha        2.58%       74.098      487,805   \n",
       "16   17                      Assam        1.67%       47.982      315,881   \n",
       "17   18               Chhattisgarh        1.61%       46.187      304,063   \n",
       "18   19                  Jharkhand        1.57%       45.145      297,204   \n",
       "19   20                Uttarakhand        1.30%       37.351      245,895   \n",
       "20   21            Jammu & Kashmir        0.83%       23.690      155,956   \n",
       "21   22           Himachal Pradesh        0.81%       23.369      153,845   \n",
       "22   23                        Goa        0.39%       11.115       73,170   \n",
       "23   24                    Tripura        0.26%        7.571       49,845   \n",
       "24   25                 Chandigarh        0.22%        6.397       42,114   \n",
       "25   26                 Puducherry        0.18%        5.230       34,433   \n",
       "26   27                  Meghalaya        0.18%        5.086       33,481   \n",
       "27   28                     Sikkim        0.15%        4.363       28,723   \n",
       "28   29                    Manipur        0.15%        4.233       27,870   \n",
       "29   30                   Nagaland        0.14%        4.144       27,283   \n",
       "30   31          Arunachal Pradesh        0.13%        3.737       24,603   \n",
       "31   32                    Mizoram        0.12%        3.385       22,287   \n",
       "32   33  Andaman & Nicobar Islands            -            -            -   \n",
       "\n",
       "   GSDP_Previous  \n",
       "0      2,039,074  \n",
       "1      1,215,307  \n",
       "2      1,123,982  \n",
       "3      1,186,379  \n",
       "4      1,091,077  \n",
       "5        739,525  \n",
       "6        677,428  \n",
       "7        621,301  \n",
       "8        612,828  \n",
       "9        522,009  \n",
       "10       559,412  \n",
       "11       590,569  \n",
       "12       531,085  \n",
       "13       375,651  \n",
       "14       397,669  \n",
       "15       376,877  \n",
       "16       234,048  \n",
       "17       231,182  \n",
       "18       224,986  \n",
       "19       193,273  \n",
       "20       112,755  \n",
       "21       117,851  \n",
       "22        57,787  \n",
       "23        36,963  \n",
       "24        31,192  \n",
       "25        23,013  \n",
       "26        24,682  \n",
       "27        18,722  \n",
       "28        19,300  \n",
       "29        17,647  \n",
       "30        16,676  \n",
       "31        16,478  \n",
       "32             -  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"http://statisticstimes.com/economy/india/indian-states-gdp.php\")\n",
    "\n",
    "Rank=[]\n",
    "State =[]\n",
    "GDP=[]\n",
    "GSDP_Current=[]\n",
    "GSDP_Previous=[]\n",
    "Share=[]\n",
    "\n",
    "#scraping the Rank \n",
    "r=driver.find_elements(By.XPATH,\"//td[@class='data1']\")\n",
    "for i in r:\n",
    "    if i.text is None :\n",
    "        Rank.append(\"--\") \n",
    "    else:\n",
    "        Rank.append(i.text)\n",
    "St=driver.find_elements(By.XPATH,\"//td[@class='name']\")\n",
    "for i in St:\n",
    "    if i.text is None :\n",
    "        State.append(\"--\") \n",
    "    else:\n",
    "        State.append(i.text)\n",
    "gdp=driver.find_elements(By.XPATH,\"//*[@id='table_id']/tbody/tr/td[6]\")\n",
    "for i in gdp:\n",
    "    if i.text is None :\n",
    "        GDP.append(\"--\") \n",
    "    else:\n",
    "        GDP.append(i.text)\n",
    "#scraping the Share \n",
    "shr=driver.find_elements(By.XPATH,\"//*[@id='table_id']/tbody/tr/td[5]\")\n",
    "for i in shr:\n",
    "    if i.text is None :\n",
    "        Share.append(\"--\") \n",
    "    else:\n",
    "        Share.append(i.text)\n",
    "#scraping the GSDP_Current \n",
    "shr=driver.find_elements(By.XPATH,\"//*[@id='table_id']/tbody/tr/td[4]\")\n",
    "for i in shr:\n",
    "    if i.text is None :\n",
    "        GSDP_Current.append(\"--\") \n",
    "    else:\n",
    "        GSDP_Current.append(i.text)\n",
    "#scraping the GSDP_Previous \n",
    "shr=driver.find_elements(By.XPATH,\"//*[@id='table_id']/tbody/tr/td[8]\")\n",
    "for i in shr:\n",
    "    if i.text is None :\n",
    "        GSDP_Previous.append(\"--\") \n",
    "    else:\n",
    "        GSDP_Previous.append(i.text)\n",
    "        \n",
    "#Creating Dataframe\n",
    "State_GDP=pd.DataFrame([])\n",
    "State_GDP['Rank']=Rank[:33]\n",
    "State_GDP['State']=State[:33]\n",
    "State_GDP['Share In GDP']=Share[:33]\n",
    "State_GDP['GDP of State']=GDP[:33]\n",
    "State_GDP['GSDP_Current']=GSDP_Current[:33]\n",
    "State_GDP['GSDP_Previous']=GSDP_Previous[:33]\n",
    "State_GDP        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "31c73444",
   "metadata": {},
   "source": [
    "4. Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7de2dc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trending repositories section not found.\n"
     ]
    }
   ],
   "source": [
    "# Set up Selenium webdriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://github.com/\")\n",
    "\n",
    "try:\n",
    "    # Find the trending repositories section\n",
    "    trending_repos = driver.find_element(By.XPATH, \"//div[@class='explore-pjax-container container-lg p-responsive pt-6']\")\n",
    "    \n",
    "    # Find all repository cards within the trending repositories section\n",
    "    repo_cards = trending_repos.find_elements(By.XPATH, \".//article[@class='Box-row']\")\n",
    "\n",
    "    for card in repo_cards:\n",
    "        try:\n",
    "            # Find repository title\n",
    "            title = card.find_element(By.XPATH, \".//h1/a\").text\n",
    "            \n",
    "            # Find repository description\n",
    "            description = card.find_element(By.XPATH, \".//p\").text\n",
    "            \n",
    "            # Find contributors count\n",
    "            contributors_count = card.find_element(By.XPATH, \".//a[contains(@href, '/contributors')]\").text\n",
    "            \n",
    "            # Find language used (if available)\n",
    "            try:\n",
    "                language_used = card.find_element(By.XPATH, \".//span[@itemprop='programmingLanguage']\").text\n",
    "            except NoSuchElementException:\n",
    "                language_used = \"Not specified\"\n",
    "            \n",
    "            print(\"Repository Title:\", title)\n",
    "            print(\"Repository Description:\", description)\n",
    "            print(\"Contributors Count:\", contributors_count)\n",
    "            print(\"Language Used:\", language_used)\n",
    "            \n",
    "        except NoSuchElementException:\n",
    "            continue\n",
    "\n",
    "except NoSuchElementException:\n",
    "    print(\"Trending repositories section not found.\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d3d31fb",
   "metadata": {},
   "source": [
    "5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/ You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf4983e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song_Name</th>\n",
       "      <th>Peak_rank</th>\n",
       "      <th>Singer / Crew</th>\n",
       "      <th>Last_Week_Rank</th>\n",
       "      <th>Weeks_on_board</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paint The Town Red</td>\n",
       "      <td>1</td>\n",
       "      <td>Doja Cat</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Snooze</td>\n",
       "      <td>2</td>\n",
       "      <td>SZA</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cruel Summer</td>\n",
       "      <td>4</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fast Car</td>\n",
       "      <td>3</td>\n",
       "      <td>Luke Combs</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3D</td>\n",
       "      <td>-</td>\n",
       "      <td>Jung Kook &amp; Jack Harlow</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Long Journey</td>\n",
       "      <td>75</td>\n",
       "      <td>Rod Wave</td>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>But I Got A Beer In My Hand</td>\n",
       "      <td>-</td>\n",
       "      <td>Luke Bryan</td>\n",
       "      <td>92</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Rubicon</td>\n",
       "      <td>-</td>\n",
       "      <td>Peso Pluma</td>\n",
       "      <td>63</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>East Side Of Sorrow</td>\n",
       "      <td>95</td>\n",
       "      <td>Zach Bryan</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Where She Goes</td>\n",
       "      <td>-</td>\n",
       "      <td>Bad Bunny</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Song_Name Peak_rank            Singer / Crew  \\\n",
       "0            Paint The Town Red         1                 Doja Cat   \n",
       "1                        Snooze         2                      SZA   \n",
       "2                  Cruel Summer         4             Taylor Swift   \n",
       "3                      Fast Car         3               Luke Combs   \n",
       "4                            3D         -  Jung Kook & Jack Harlow   \n",
       "..                          ...       ...                      ...   \n",
       "95                 Long Journey        75                 Rod Wave   \n",
       "96  But I Got A Beer In My Hand         -               Luke Bryan   \n",
       "97                      Rubicon         -               Peso Pluma   \n",
       "98          East Side Of Sorrow        95               Zach Bryan   \n",
       "99               Where She Goes         -                Bad Bunny   \n",
       "\n",
       "   Last_Week_Rank Weeks_on_board  \n",
       "0               1              9  \n",
       "1               2             43  \n",
       "2               3             22  \n",
       "3               2             28  \n",
       "4               5              1  \n",
       "..            ...            ...  \n",
       "95             39              3  \n",
       "96             92              3  \n",
       "97             63             12  \n",
       "98             18              6  \n",
       "99              8             19  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"https://www.billboard.com/charts/hot-100\")\n",
    "\n",
    "Song_Name =[]\n",
    "Singer=[]\n",
    "rank=[]\n",
    "Last_Week=[]\n",
    "Weeks_on_board=[]\n",
    "\n",
    "#scraping the Song_Name \n",
    "son=driver.find_elements(By.XPATH,\"//li[@Class='lrv-u-width-100p']\")\n",
    "for i in son:\n",
    "    if i.text is None :\n",
    "        Song_Name.append(\"--\") \n",
    "        Singer.append(\"--\") \n",
    "        rank.append(\"--\") \n",
    "        Last_Week.append(\"--\") \n",
    "        Weeks_on_board.append(\"--\") \n",
    "    else:\n",
    "        Song_Name.append(i.text.split('\\n')[0])\n",
    "        Singer.append(i.text.split('\\n')[1])\n",
    "        rank.append(i.text.split('\\n')[2])\n",
    "        Last_Week.append(i.text.split('\\n')[3])\n",
    "        Weeks_on_board.append(i.text.split('\\n')[4])\n",
    "\n",
    "\n",
    "BillBOARD_HOT100=pd.DataFrame([])\n",
    "BillBOARD_HOT100['Song_Name']=Song_Name\n",
    "BillBOARD_HOT100['Peak_rank']=rank\n",
    "BillBOARD_HOT100['Singer / Crew']=Singer\n",
    "BillBOARD_HOT100['Last_Week_Rank']=Last_Week\n",
    "BillBOARD_HOT100['Weeks_on_board']=Weeks_on_board\n",
    "\n",
    "BillBOARD_HOT100"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6984b9a5",
   "metadata": {},
   "source": [
    "6. Scrape the details of Highest selling novels.\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre\n",
    "Url - https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f97d176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book_name</th>\n",
       "      <th>Author_name</th>\n",
       "      <th>Volumes_sold</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Book_name       Author_name  \\\n",
       "0                                   Da Vinci Code,The        Brown, Dan   \n",
       "1                Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2            Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3           Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4                                Fifty Shades of Grey      James, E. L.   \n",
       "..                                                ...               ...   \n",
       "95                                          Ghost,The    Harris, Robert   \n",
       "96                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "   Volumes_sold        Publisher                        Genre  \n",
       "0     5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1     4,475,152       Bloomsbury           Children's Fiction  \n",
       "2     4,200,654       Bloomsbury           Children's Fiction  \n",
       "3     4,179,479       Bloomsbury           Children's Fiction  \n",
       "4     3,758,936     Random House              Romance & Sagas  \n",
       "..          ...              ...                          ...  \n",
       "95      807,311     Random House   General & Literary Fiction  \n",
       "96      794,201          Penguin        Food & Drink: General  \n",
       "97      792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98      791,507            Orion           Biography: General  \n",
       "99      791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver=webdriver.Chrome() \n",
    "driver.get(\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/\")\n",
    "time.sleep(6)\n",
    "\n",
    "\n",
    "# creating empty lists for scraping data\n",
    "Book_name=[]\n",
    "Author_name=[]\n",
    "Volumes_sold=[]\n",
    "Publisher=[]\n",
    "Genre=[]\n",
    "\n",
    "# Scraping Book_name\n",
    "book_name=driver.find_elements(By.XPATH,\"//table[@class='in-article sortable']/tbody/tr/td[2]\")\n",
    "for i in book_name:\n",
    "    if i.text is None:\n",
    "        Book_name.append('--')\n",
    "    else:\n",
    "        Book_name.append(i.text)\n",
    "        \n",
    "# Scraping Author_name\n",
    "author_name=driver.find_elements(By.XPATH,\"//table[@class='in-article sortable']/tbody/tr/td[3]\")\n",
    "for i in author_name:\n",
    "    if i.text is None:\n",
    "        Author_name.append('--')\n",
    "    else:\n",
    "        Author_name.append(i.text)\n",
    "        \n",
    "# Scraping Volumes_sold\n",
    "volumes_sold=driver.find_elements(By.XPATH,\"//table[@class='in-article sortable']/tbody/tr/td[4]\")\n",
    "for i in volumes_sold:\n",
    "    if i.text is None:\n",
    "        Volumes_sold.append('--')\n",
    "    else:\n",
    "        Volumes_sold.append(i.text)\n",
    "        \n",
    "# Scraping Author_name\n",
    "publisher=driver.find_elements(By.XPATH,\"//table[@class='in-article sortable']/tbody/tr/td[5]\")\n",
    "for i in publisher:\n",
    "    if i.text is None:\n",
    "        Publisher.append('--')\n",
    "    else:\n",
    "        Publisher.append(i.text)\n",
    "        \n",
    "# Scraping Genre\n",
    "genre=driver.find_elements(By.XPATH,\"//table[@class='in-article sortable']/tbody/tr/td[6]\")\n",
    "for i in genre:\n",
    "    if i.text is None:\n",
    "        Genre.append('--')\n",
    "    else:\n",
    "        Genre.append(i.text)\n",
    "\n",
    "Top_selling_book=pd.DataFrame({})\n",
    "Top_selling_book['Book_name']=Book_name\n",
    "Top_selling_book['Author_name']=Author_name\n",
    "Top_selling_book['Volumes_sold']=Volumes_sold\n",
    "Top_selling_book['Publisher']=Publisher\n",
    "Top_selling_book['Genre']=Genre\n",
    "Top_selling_book"
   ]
  },
  {
   "cell_type": "raw",
   "id": "988e5be1",
   "metadata": {},
   "source": [
    "7. Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/ You have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9b1e511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year_span</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Run_time</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>(2011–2019)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>57 min</td>\n",
       "      <td>9.2</td>\n",
       "      <td>2,211,556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>(2016–2025)</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1,281,741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>(2010–2022)</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1,049,264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13 Reasons Why</td>\n",
       "      <td>(2017–2020)</td>\n",
       "      <td>Drama, Mystery, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>308,350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 100</td>\n",
       "      <td>(2014–2020)</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>267,327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Reign</td>\n",
       "      <td>(2013–2017)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>42 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>52,910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A Series of Unfortunate Events</td>\n",
       "      <td>(2017–2019)</td>\n",
       "      <td>Adventure, Comedy, Drama</td>\n",
       "      <td>50 min</td>\n",
       "      <td>7.8</td>\n",
       "      <td>64,977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Criminal Minds</td>\n",
       "      <td>(2005– )</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>42 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>211,725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Scream: The TV Series</td>\n",
       "      <td>(2015–2019)</td>\n",
       "      <td>Comedy, Crime, Drama</td>\n",
       "      <td>45 min</td>\n",
       "      <td>7</td>\n",
       "      <td>44,050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The Haunting of Hill House</td>\n",
       "      <td>(2018)</td>\n",
       "      <td>Drama, Horror, Mystery</td>\n",
       "      <td>572 min</td>\n",
       "      <td>8.6</td>\n",
       "      <td>268,193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Name    Year_span                     Genre  \\\n",
       "0                  Game of Thrones  (2011–2019)  Action, Adventure, Drama   \n",
       "1                  Stranger Things  (2016–2025)    Drama, Fantasy, Horror   \n",
       "2                 The Walking Dead  (2010–2022)   Drama, Horror, Thriller   \n",
       "3                   13 Reasons Why  (2017–2020)  Drama, Mystery, Thriller   \n",
       "4                          The 100  (2014–2020)    Drama, Mystery, Sci-Fi   \n",
       "..                             ...          ...                       ...   \n",
       "95                           Reign  (2013–2017)                     Drama   \n",
       "96  A Series of Unfortunate Events  (2017–2019)  Adventure, Comedy, Drama   \n",
       "97                  Criminal Minds     (2005– )     Crime, Drama, Mystery   \n",
       "98           Scream: The TV Series  (2015–2019)      Comedy, Crime, Drama   \n",
       "99      The Haunting of Hill House       (2018)    Drama, Horror, Mystery   \n",
       "\n",
       "   Run_time Ratings      Votes  \n",
       "0    57 min     9.2  2,211,556  \n",
       "1    51 min     8.7  1,281,741  \n",
       "2    44 min     8.1  1,049,264  \n",
       "3    60 min     7.5    308,350  \n",
       "4    43 min     7.6    267,327  \n",
       "..      ...     ...        ...  \n",
       "95   42 min     7.5     52,910  \n",
       "96   50 min     7.8     64,977  \n",
       "97   42 min     8.1    211,725  \n",
       "98   45 min       7     44,050  \n",
       "99  572 min     8.6    268,193  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver=webdriver.Chrome() \n",
    "driver.get(\"https://www.imdb.com/list/ls095964455/\")\n",
    "\n",
    "\n",
    "# Creating empty list\n",
    "Name=[]\n",
    "Year_span=[]\n",
    "Genre=[]\n",
    "Run_time=[]\n",
    "Ratings=[]\n",
    "Votes=[]\n",
    "\n",
    "#Scraping Name\n",
    "name=driver.find_elements(By.XPATH,\"//h3[@class='lister-item-header']/a\")\n",
    "for i in name:\n",
    "    Name.append(i.text)\n",
    "    \n",
    "#Scraping Year_span\n",
    "year_span=driver.find_elements(By.XPATH,\"//h3[@class='lister-item-header']/span[2]\")\n",
    "for i in year_span:\n",
    "    Year_span.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping Genre\n",
    "genre=driver.find_elements(By.XPATH,\"//p[@class='text-muted text-small']/span[5]\")\n",
    "for i in genre:\n",
    "    Genre.append(i.text)\n",
    "    \n",
    "    \n",
    "#Scraping Run_time\n",
    "run_time=driver.find_elements(By.XPATH,\"//p[@class='text-muted text-small']/span[3]\")\n",
    "for i in run_time:\n",
    "    Run_time.append(i.text)\n",
    "    \n",
    "#Scraping Ratings\n",
    "ratings=driver.find_elements(By.XPATH,\"//div[@class='ipl-rating-widget']/div/span[2]\")\n",
    "for i in ratings:\n",
    "    Ratings.append(i.text)\n",
    "    \n",
    "#Scraping Votes\n",
    "votes=driver.find_elements(By.XPATH,\"//p[@class='text-muted text-small'][3]/span[2]\")\n",
    "for i in votes:\n",
    "    Votes.append(i.text)\n",
    "    \n",
    "Top_shows=pd.DataFrame({})\n",
    "Top_shows['Name']=Name\n",
    "Top_shows['Year_span']=Year_span\n",
    "Top_shows['Genre']=Genre\n",
    "Top_shows['Run_time']=Run_time\n",
    "Top_shows['Ratings']=Ratings\n",
    "Top_shows['Votes']=Votes\n",
    "Top_shows"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd3752cf",
   "metadata": {},
   "source": [
    "8. Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/ You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute G) Year\n",
    "Note: - from the home page you have to go to the Show All Dataset page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99dcf868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Importing selenium webdriver \n",
    "from selenium import webdriver\n",
    "\n",
    "# Importing required Exceptions which needs to handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "#Importing requests\n",
    "import requests\n",
    "\n",
    "# importing regex\n",
    "import re\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6eed1ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome() \n",
    "driver.get(\"https://archive.ics.uci.edu/\")\n",
    "time.sleep(6)\n",
    "\n",
    "\n",
    "view_datasets= driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[1]/div/div/div/a[1]')\n",
    "view_datasets.click()\n",
    "time.sleep(4)\n",
    "\n",
    "# creating empty lists for scraping data\n",
    "Dataset_name=[]\n",
    "Data_type=[]\n",
    "Task=[]\n",
    "Attribute_type=[]\n",
    "No_of_instances=[]\n",
    "No_of_attribute=[]\n",
    "Year=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "843ac1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_all= driver.find_element(By.XPATH,'/html/body/div/div[1]/div[1]/main/div/div[2]/div[1]/div/label[2]')\n",
    "expand_all.click()\n",
    "time.sleep(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "352b617b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris\n",
      "Heart Disease\n",
      "Adult\n",
      "Wine\n",
      "Diabetes\n",
      "Dry Bean Dataset\n",
      "Breast Cancer Wisconsin (Diagnostic)\n",
      "Car Evaluation\n",
      "Rice (Cammeo and Osmancik)\n",
      "Mushroom\n"
     ]
    }
   ],
   "source": [
    "#scraping Dataset name\n",
    "data_name= driver.find_elements(By.XPATH,'//a[@class=\"link-hover link text-xl font-semibold\"]')\n",
    "for i in data_name:\n",
    "    if i.text is None :\n",
    "        Dataset_name.append(\"--\") \n",
    "    else:\n",
    "        Dataset_name.append(i.text)\n",
    "        print(i.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34745bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multivariate\n",
      "Multivariate\n",
      "Tabular\n",
      "\n",
      "Multivariate\n",
      "Multivariate\n",
      "Multivariate\n",
      "Multivariate\n",
      "Multivariate\n"
     ]
    }
   ],
   "source": [
    "time.sleep(4)        \n",
    "#Scraping data Type\n",
    "data_type=driver.find_elements(By.XPATH,'//div[@class=\"my-2 hidden gap-4 md:grid grid-cols-12\"]//div[2]//span')\n",
    "for i in data_type[1:]:\n",
    "    if i.text is None :\n",
    "        Data_type.append(\"--\") \n",
    "    else:\n",
    "        Data_type.append(i.text)\n",
    "        print(i.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1561e64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification\n",
      "Classification\n",
      "Classification\n",
      "\n",
      "Classification\n",
      "Classification\n",
      "Classification\n",
      "Classification\n",
      "Classification\n"
     ]
    }
   ],
   "source": [
    "#Scraping Task\n",
    "task=driver.find_elements(By.XPATH,'//div[@class=\"my-2 hidden gap-4 md:grid grid-cols-12\"]//div[1]//span')\n",
    "for i in task[1:]:\n",
    "    if i.text is None :\n",
    "        Task.append(\"--\") \n",
    "    else:\n",
    "        Task.append(i.text)\n",
    "        print(i.text)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c59ae91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life Science\n",
      "Social Science\n",
      "Physical Science\n",
      "Life Science\n",
      "Computer Science\n",
      "Life Science\n",
      "Other\n",
      "Computer Science\n",
      "Life Science\n"
     ]
    }
   ],
   "source": [
    "#Scraping Attribute_type\n",
    "attribute_type=driver.find_elements(By.XPATH,'//table[@class=\"col-span-full my-2 table sm:col-start-2\"]//tr//td[1]')\n",
    "for i in attribute_type[1:]:\n",
    "    if i.text is None :\n",
    "        Attribute_type.append(\"--\") \n",
    "    else:\n",
    "        Attribute_type.append(i.text)\n",
    "        print(i.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9575e771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 Instances\n",
      "48.84K Instances\n",
      "178 Instances\n",
      "\n",
      "13.61K Instances\n",
      "569 Instances\n",
      "1.73K Instances\n",
      "3.81K Instances\n",
      "8.12K Instances\n"
     ]
    }
   ],
   "source": [
    "#Scraping No_of_instances\n",
    "no_of_instances=driver.find_elements(By.XPATH,'//div[@class=\"my-2 hidden gap-4 md:grid grid-cols-12\"]//div[3]//span')\n",
    "for i in no_of_instances[1:]:\n",
    "    if i.text is None :\n",
    "        No_of_instances.append(\"--\") \n",
    "    else:\n",
    "        No_of_instances.append(i.text)\n",
    "        print(i.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32e2c72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 Features\n",
      "14 Features\n",
      "13 Features\n",
      "20 Features\n",
      "16 Features\n",
      "30 Features\n",
      "6 Features\n",
      "8 Features\n",
      "22 Features\n"
     ]
    }
   ],
   "source": [
    "#Scraping No_of_attribute\n",
    "no_of_attribute=driver.find_elements(By.XPATH,'//div[@class=\"my-2 hidden gap-4 md:grid grid-cols-12\"]//div[4]//span')\n",
    "for i in no_of_attribute[1:]:\n",
    "    if i.text is None :\n",
    "        No_of_attribute.append(\"--\") \n",
    "    else:\n",
    "        No_of_attribute.append(i.text)\n",
    "        print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0cdf129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/1/1988\n",
      "5/1/1996\n",
      "7/1/1991\n",
      "N/A\n",
      "9/14/2020\n",
      "11/1/1995\n",
      "6/1/1997\n",
      "10/6/2019\n",
      "4/27/1987\n"
     ]
    }
   ],
   "source": [
    "#Scraping Year\n",
    "year=driver.find_elements(By.XPATH,'//table[@class=\"col-span-full my-2 table sm:col-start-2\"]//tr//td[3]')\n",
    "for i in year[1:]:\n",
    "    if i.text is None :\n",
    "        Year.append(\"--\") \n",
    "    else:\n",
    "        Year.append(i.text)\n",
    "        print(i.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7bd1d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Name: Iris\n",
      "Data Type: Multivariate\n",
      "Task: Classification\n",
      "Attribute Type: Life Science\n",
      "No of Instances: 303 Instances\n",
      "No of Attributes: 13 Features\n",
      "Year: 7/1/1988\n",
      "====================\n",
      "Dataset Name: Heart Disease\n",
      "Data Type: Multivariate\n",
      "Task: Classification\n",
      "Attribute Type: Social Science\n",
      "No of Instances: 48.84K Instances\n",
      "No of Attributes: 14 Features\n",
      "Year: 5/1/1996\n",
      "====================\n",
      "Dataset Name: Adult\n",
      "Data Type: Tabular\n",
      "Task: Classification\n",
      "Attribute Type: Physical Science\n",
      "No of Instances: 178 Instances\n",
      "No of Attributes: 13 Features\n",
      "Year: 7/1/1991\n",
      "====================\n",
      "Dataset Name: Wine\n",
      "Data Type: \n",
      "Task: \n",
      "Attribute Type: Life Science\n",
      "No of Instances: \n",
      "No of Attributes: 20 Features\n",
      "Year: N/A\n",
      "====================\n",
      "Dataset Name: Diabetes\n",
      "Data Type: Multivariate\n",
      "Task: Classification\n",
      "Attribute Type: Computer Science\n",
      "No of Instances: 13.61K Instances\n",
      "No of Attributes: 16 Features\n",
      "Year: 9/14/2020\n",
      "====================\n",
      "Dataset Name: Dry Bean Dataset\n",
      "Data Type: Multivariate\n",
      "Task: Classification\n",
      "Attribute Type: Life Science\n",
      "No of Instances: 569 Instances\n",
      "No of Attributes: 30 Features\n",
      "Year: 11/1/1995\n",
      "====================\n",
      "Dataset Name: Breast Cancer Wisconsin (Diagnostic)\n",
      "Data Type: Multivariate\n",
      "Task: Classification\n",
      "Attribute Type: Other\n",
      "No of Instances: 1.73K Instances\n",
      "No of Attributes: 6 Features\n",
      "Year: 6/1/1997\n",
      "====================\n",
      "Dataset Name: Car Evaluation\n",
      "Data Type: Multivariate\n",
      "Task: Classification\n",
      "Attribute Type: Computer Science\n",
      "No of Instances: 3.81K Instances\n",
      "No of Attributes: 8 Features\n",
      "Year: 10/6/2019\n",
      "====================\n",
      "Dataset Name: Rice (Cammeo and Osmancik)\n",
      "Data Type: Multivariate\n",
      "Task: Classification\n",
      "Attribute Type: Life Science\n",
      "No of Instances: 8.12K Instances\n",
      "No of Attributes: 22 Features\n",
      "Year: 4/27/1987\n",
      "====================\n",
      "Dataset Name: Mushroom\n",
      "Data Type: --\n",
      "Task: --\n",
      "Attribute Type: --\n",
      "No of Instances: --\n",
      "No of Attributes: --\n",
      "Year: --\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "# Find the maximum length among all the lists\n",
    "max_length = max(len(Dataset_name), len(Data_type), len(Task), len(Attribute_type), len(No_of_instances), len(No_of_attribute), len(Year))\n",
    "\n",
    "# Iterate through the range of the maximum length\n",
    "for i in range(max_length):\n",
    "    print(f\"Dataset Name: {Dataset_name[i] if i < len(Dataset_name) else '--'}\")\n",
    "    print(f\"Data Type: {Data_type[i] if i < len(Data_type) else '--'}\")\n",
    "    print(f\"Task: {Task[i] if i < len(Task) else '--'}\")\n",
    "    print(f\"Attribute Type: {Attribute_type[i] if i < len(Attribute_type) else '--'}\")\n",
    "    print(f\"No of Instances: {No_of_instances[i] if i < len(No_of_instances) else '--'}\")\n",
    "    print(f\"No of Attributes: {No_of_attribute[i] if i < len(No_of_attribute) else '--'}\")\n",
    "    print(f\"Year: {Year[i] if i < len(Year) else '--'}\")\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f6e607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
